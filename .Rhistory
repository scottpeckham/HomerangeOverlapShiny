runApp()
runApp()
runApp()
runApp()
runApp()
herd <- "Burnt River"
outptut.proj <- or.prg
outptut.proj <- or.prj
# Table Name in DB for GPS data #
gps.tab.name <- "AnimalID_GPS"
# File with the library of functions built for all this work #
#source("HomeRange-Functions.R")
source("/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/RCode/Functions/HomeRange-Functions.R")
source("download-helpers.R")
# EPSG CRS for each state
or.prj <- 26911 # UTM11N NAD83
wa.prj <- 2285
wgs.proj <- 4326
id.prj <- 8826 # Idaho Transverse Mercator
t1 <- as.POSIXct("2024-01-01 12:00",tz="UTC")
t2 <- as.POSIXct("2024-03-01 12:00",tz="UTC")
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/BHS_TriState.db"
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
gps_db <- tbl(con, gps.tab.name) # reference to the table
input <- NULL
input$selectHerd <- "Burnt River"
input$sex <- "Female"
input$sex <- "FEMALE"
gps <- gps_db %>% filter(Herd==input$selectHerd & Sex==input$sex) %>%
filter(acquisitiontime>= t1 & acquisitiontime <= t2) %>% collect()
nrow(gps)
gps <- removeMissingGPS(gps)
# DEAL WITH THE WEIRD CASES THAT CRASH HOME RANG CALCS
# Screen off any spurious Lat/Lon values that can crash HR calcs (there was one in MT for LPMS that bombed everything #
gps <- gps %>% filter(Latitude > 43.5 & Latitude < 47.5 & Longitude > -121.5 & Longitude < -113)
# LPMS has a weirdo point so we screen out anything east of lon -113
gps <- gps %>% filter(Longitude < -113.0)
# LS has a weirdo point so we screen out anything east of lon -113
gps <- gps %>% filter(!(AnimalID == "20LS49" & Latitude < 44.4))
# remove duplicates if there are any
gps <- distinct(gps)
gps <- st_as_sf(gps, coords = c("Longitude", "Latitude"), crs = 4326)
homeranges <- suppressWarnings(calculateBBHomerange(gps,min.fixes=30,grid=400, extent=1.1,
contour.percent=50,
output.proj=output.proj,output.UD=TRUE))
output.proj <- or.prj
homeranges <- suppressWarnings(calculateBBHomerange(gps,min.fixes=30,grid=400, extent=1.1,
contour.percent=50,
output.proj=output.proj,output.UD=TRUE))
overlap <- suppressWarnings(calculateHomerangeOverlap(homeranges$homeranges)
)
community <- getClusterCommunity(overlap)
community
df <- getClusterDataFrame(community$cluster)
clusters <- sort(unique(df$Cluster))
members <- c(rep("",length(clusters)))
for (i in 1:length(clusters)){
membs <- df$AnimalID[df$Cluster==clusters[i]]
members[i] <- str_flatten(membs,collapse=", ")
}
homeranges$Cluster <- df$Cluster[which(homeranges$AnimalID %in% df$AnimalID,arr.ind=TRUE)]
homeranges <- suppressWarnings(addClusterStats(homeranges))
runApp()
runApp()
runApp()
# R script to check for mortality-like point patterns in recent GPS data,
#   in absence of getting mort notifications or for checking on things
#
#   written by S. Peckham 4/10/2024, revised 5/10/23 to move to SDD (see sfdep package) instead of MCP areas
# packages
library(collar)
library(sf)
library(RSQLite)
library(dplyr)
library(dbplyr)
library(emayili)
library(RColorBrewer)
library(adehabitatHR)
library(mapview)
library(sfdep)
library(pandoc)
##### user settings  (MODIFY FOR YOUR SYSTEM) ######
# path to db
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/BHS_TriState.db"
# gps table name in DB
gps.tab.name <- "AnimalID_GPS"
# Area threshold in m2 that we will screen minimum convex polygons by
area.threshold <- 18000 # just guessing here 150m x 150m square, a 75m radius circle is area 17670 m2
sdd.threshold <- 30 # m  in some tests, true morts had sdd < 15. So larger values might catch some live animals
# Use SDD or MCP to estimate (default to SDD as that seems to be more reliable to pick up the point pattern, MCP was easy and temp fix)
use.sdd <- TRUE   # if set to F, code changes need to be made below, I didn't intend for MCP use after implementing point pattern searches
# determine our cutoff time, how far to look back from now (days)?
ndays <- 7 # needs to be >=3 for most 13hr data or we'll have too few locations
# Also want to look at most recent N locations, for collars that have failed or send infrequent fixes
lastN <- 10
# location to store HTML map file
html.path <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/GPS_data/MortPatternChecks"
admin.email <- "scott.peckham78@gmail.com"
#or.emails <- c("Brian.S.RATLIFF@odfw.oregon.gov","frances.cassirer@idfg.idaho.gov")
#wa.emails <- c("Erin.Wampole@dfw.wa.gov","William.Moore@dfw.wa.gov",
#               "Carrie.Kyle@dfw.wa.gov","frances.cassirer@idfg.idaho.gov")
source("/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/RCode/Functions/HomeRange-Functions.R")
### end user settings #######
# Configure the start date and time
days <- 60*60*24*ndays # seconds in a week
start.time <- Sys.time()-days
start_date <- format(start.time,format="%Y-%m-%d %H:%M:%S",tz="UTC")
# load GPS data
# Connect to the data base read GPS table we need #
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
# query for gps, don't read into memory
gps_db <- tbl(con, gps.tab.name) # reference to the table
# execute queries store in data frame
gps <- gps_db %>% filter(acquisitiontime >= start_date) %>% collect() # last ndays of locations for all animals
gpsL10 <- FetchLastNFixes(gps_db,lastN)  # most recent N locations for all animals
gpsL10 <- gpsL10 %>% filter(CollarEndType==0) # just keep animals we think are alive and active
# close out DB connection
dbDisconnect(con)
# convert GPS locations to spatial feature
gps <- removeMissingGPS(gps)
gpsL10 <- removeMissingGPS(gpsL10)
gps.sf <- st_as_sf(gps, coords = c("Longitude", "Latitude"), crs = 4326)
gpsL10.sf <- st_as_sf(gpsL10, coords = c("Longitude", "Latitude"), crs = 4326)
# project to a coord system all herds can use (start with a albers for US epsg 102003)
gps.prj <- st_transform(gps.sf,crs=5070)
gpsL10.prj <- st_transform(gpsL10.sf,crs=5070)
if (use.sdd){
m.check <- FindMortPattern_bySDD(gps.prj,sdd.threshold)
mL10.check <- FindMortPattern_bySDD(gpsL10.prj,sdd.threshold)
} else {
# check for points that create an MCP with area < area.threshold
m.check <- FindMortPattern_byMCP(gps.prj,area.threshold)
mL10.check <- FindMortPattern_byMCP(gpsL10.prj,area.threshold)
}
# condition on if we've detected mort-like patterns
if (nrow(m.check) > 0) {
print(paste("Number of possible mortalities detected:",nrow(m.check)))
# create a quick map of this for review #
p.colors <- brewer.pal(9,"Set1")[1:nrow(m.check)]
#Last7Days <- mcp.check$PointsOut
Last7Days <- gps.prj %>% filter(AnimalID %in% m.check$AnimalID)
#fmap <- makeLinePointMap(a.pts)+mapview(hrange.list,col.regions = brewer.pal(nherds, "Set1"), alpha.regions=0.6)
fmap <- mapview(Last7Days,zcol="AnimalID",col.regions=p.colors,alpha.regions=0.8)
html_fl = paste(html.path,"/MortDetection_Recent",ndays,"_Days",Sys.Date(),".html",sep='')
#mapshot(fmap, url = html_fl, selfcontained = FALSE)
mapshot(fmap, url = html_fl)
# notify by email #
# who needs to recieve it? #
to <- admin.email #always sent to admin
# if (any(or.herds %in% fherds)) to <- c(to,or.emails)
# if (any(wa.herds %in% fherds)) to <- c(to,wa.emails)
# to <- unique(to) # remove duplicates, if any
subject.text <- paste("BHS Possible Mort Notification",Sys.Date())
body.text <- paste("Potential mort detected for AnimalID(s) in recent fixes:", paste(unique(Last7Days$AnimalID),collapse=' '))
email <- emayili::envelope(
to = to,
from = "scott.peckham78@gmail.com",
subject = subject.text,
text = body.text
)
smtp <- server(host = "smtp.gmail.com",
port = 465,
username = "scott.peckham78@gmail.com",
password = "smjy uhvt wfeo naxk")
email <- email %>% attachment(html_fl)
smtp(email, verbose = TRUE)
} else print(paste(Sys.Date(),"No mort patterns detected in most recent location data.")) # end if statement
if (nrow(mL10.check) > 0) {
print(paste("Number of possible mortalities detected in last N-fixes data:",nrow(mL10.check)))
# create a quick map of this for review #
#p.colors <- brewer.pal(9,"Set1")[1:nrow(mcpL10.check$MCP)]
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
qual.colors <- sample(col_vector, nrow(mL10.check))
#LastNFixes <- mcpL10.check$PointsOut
LastNFixes <- gpsL10.prj %>% filter(AnimalID %in% mL10.check$AnimalID)
#fmap <- makeLinePointMap(a.pts)+mapview(hrange.list,col.regions = brewer.pal(nherds, "Set1"), alpha.regions=0.6)
fmap <- mapview(LastNFixes,zcol="AnimalID",col.regions=qual.colors,alpha.regions=0.8)
html_fl = paste(html.path,"/MortDetection_Last_",lastN,"_Locations",Sys.Date(),".html",sep='')
mapshot(fmap, url = html_fl)
# notify by email #
# who needs to recieve it? #
to <- admin.email #always sent to admin
# if (any(or.herds %in% fherds)) to <- c(to,or.emails)
# if (any(wa.herds %in% fherds)) to <- c(to,wa.emails)
# to <- unique(to) # remove duplicates, if any
subject.text <- paste("BHS Possible Mort Notification",Sys.Date())
body.text <- paste("From last ",lastN," fixes, potential mort for AnimalID(s):", paste(unique(LastNFixes$AnimalID),collapse=' '))
email <- emayili::envelope(
to = to,
from = "scott.peckham78@gmail.com",
subject = subject.text,
text = body.text
)
smtp <- server(host = "smtp.gmail.com",
port = 465,
username = "scott.peckham78@gmail.com",
password = "smjy uhvt wfeo naxk")
email <- email %>% attachment(html_fl)
smtp(email, verbose = TRUE)
} else print(paste(Sys.Date(),"No mort patterns detected in last N fixes data.")) # end if statement
# some parameter for our query #
herd <- "Yakima Canyon"
start_day <- as.POSIXct("2024-05-15 00:01:00", tz="GMT")
end_day <- as.POSIXct("2024-05-31 23:59:00", tz="GMT")
start_day
# output file
outfile <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/GIS_Data/TriState_BHS_GPS.gpkg"
# path to main DB
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/BHS_TriState.db"
# connect to the database
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
# query for gps, don't read into memory
gps.tab.name <- "AnimalID_GPS"
gps_db <- tbl(con, gps.tab.name) # reference to the table
# query and store in data frame
herds <- gps_db %>% select(Herd) %>% collect() %>% unique()
herds
nrow(herds)
herds[1]
herds[1,1]
herds[2,1]
# output file
outfile <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/GIS_Data/TriState_BHS_GPS.gpkg"
# path to main DB
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/BHS_TriState.db"
# connect to the database
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
# query for gps, don't read into memory
gps.tab.name <- "AnimalID_GPS"
gps_db <- tbl(con, gps.tab.name) # reference to the table
# query and store in data frame
herds <- gps_db %>% select(Herd) %>% collect() %>% unique()
for (i in 1:nrow(herds)){
gps <- gps_db %>% filter(Herd==herd[i,1]) %>% collect()
# add CRS
gps <- st_as_sf(gps, coords = c("Longitude", "Latitude"), crs = 4326)
lyr <- paste(herd[i,1],"_GPS",sep='')
st_write(gps,outfile,layer=lyr, layer_options = "OVERWRITE=true")
}
# query and store in data frame
herd <- gps_db %>% select(Herd) %>% collect() %>% unique()
for (i in 1:nrow(herds)){
gps <- gps_db %>% filter(Herd==herd[i,1]) %>% collect()
# add CRS
gps <- st_as_sf(gps, coords = c("Longitude", "Latitude"), crs = 4326)
lyr <- paste(herd[i,1],"_GPS",sep='')
st_write(gps,outfile,layer=lyr, layer_options = "OVERWRITE=true")
}
herd
herd %>% as.data.frame()
herd <- herd %>% as.data.frame()
herd
herd[1]
herd[1,1]
herd[2,1]
# query and store in data frame
herd <- gps_db %>% select(Herd) %>% collect() %>% unique() %>% as.data.frame()
for (i in 1:nrow(herds)){
h <- herds[i,1]
gps <- gps_db %>% filter(Herd==h) %>% collect()
# add CRS
gps <- st_as_sf(gps, coords = c("Longitude", "Latitude"), crs = 4326)
lyr <- paste(h,"_GPS",sep='')
st_write(gps,outfile,layer=lyr, layer_options = "OVERWRITE=true")
}
i
h <- herds[i,1]
h
h <- herd[i,1]
gps <- gps_db %>% filter(Herd==h) %>% collect()
# add CRS
gps <- st_as_sf(gps, coords = c("Longitude", "Latitude"), crs = 4326)
# add CRS
gps <- st_as_sf(removeMissingGPS(gps), coords = c("Longitude", "Latitude"), crs = 4326)
lyr <- paste(h,"_GPS",sep='')
st_write(gps,outfile,layer=lyr, layer_options = "OVERWRITE=true")
for (i in 1:nrow(herds)){
h <- herd[i,1]
gps <- gps_db %>% filter(Herd==h) %>% collect()
# add CRS
gps <- st_as_sf(removeMissingGPS(gps), coords = c("Longitude", "Latitude"), crs = 4326)
lyr <- paste(h,"_GPS",sep='')
st_write(gps,outfile,layer=lyr, layer_options = "OVERWRITE=true")
}
for (i in 1:nrow(herds)){
h <- herd[i,1]
gps <- gps_db %>% filter(Herd==h) %>% collect()
# add CRS
gps <- st_as_sf(removeMissingGPS(gps), coords = c("Longitude", "Latitude"), crs = 4326)
lyr <- paste(h,"_GPS",sep='')
st_write(gps,outfile,layer=lyr, delete_layer=TRUE)
}
# close out the connection
dbDisconnect(con)
# required packages #
library(collar)
#library(odbc)
library(RSQLite)
library(dplyr)
library(dbplyr)
# Full path to main testing, finished GPS db
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/WA_DB/BHS_WA.db"
# db to house raw GPS
gps.dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/WA_DB/GPS-Raw.db"
# deployment table name
cdep.tname <- "Yakima_Cleman_Collar_Deployment"
# User supplied directory where .keyx files are stored #
keydir <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/GPS_data/CollarKeys"
all.keys <- list.files(keydir)
vec.table.name.active <- "VectronicGPS_All"
# For the crontab log file
print(Sys.time())
# open DB connections
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
gps.con <- dbConnect(RSQLite::SQLite(),gps.dbpath, extended_types=TRUE)
# read tables
cdep <- dbReadTable(con,cdep.tname)
# read tables
cdep <- dbReadTable(con,cdep.tname)
# get list of animals we need to fetch (getting everything for now, but could filter, i.e. End_Type=0)
active.fetch.animals <- cdep %>% filter(CollarType=="GPS") %>% select(AnimalID)
# deployment records of interest ()
active.fetch.cdep <- cdep %>% filter(AnimalID %in% active.fetch.animals$AnimalID)
# Download and write new Vectronic
active.vec.fetch <- active.fetch.cdep %>% filter(Manufacturer=="Vectronic" | Manufacturer=="VECTRONIC" | Manufacturer=="Vectronics")
key.matches <- unique (grep(paste(active.vec.fetch$Serialnumber,collapse="|"),
all.keys, value=TRUE))
active.key.paths <- paste(keydir,key.matches,sep="/")
a.vec.dat <- fetch_vectronics(active.key.paths, type = "gps")
a.vec.dat <- a.vec.dat %>% select(c(idposition,idcollar,acquisitiontime,scts,latitude,longitude,height,dop))
dbWriteTable(gps.con, vec.table.name.active, a.vec.dat, overwrite=TRUE)
# close out DB connection
dbDisconnect(gps.con)
dbDisconnect(con)
devtools::install_github("huh/collar")
library(devtools)
install.packages("devtools")
library(devtools)
devtools::install_github("huh/collar")
remotes::install_github("Huh/collar", ref = "develop")
# R script to read the manufacturer-based tables in the DB (serialNos only) and
#  create a unified GPS table for all AnimalIDs based on our deployment table records
#
#   written 2/23/24 to update AnimalID_GPS_toDB.R
#
#   revised 4/19/24 to deal with appending values to 'ended' tables that have location float round-off errors for same fix times
#     doing away with the 'ended' and 'active' table postfixes and added the EndType code to the table for query/filter
library(RSQLite)
library(dplyr)
# SETTINGS:  User-supplied inputs, update for your use
# Full path to main testing, finished GPS db
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/WA_DB/BHS_WA.db"
# db to house raw GPS
gps.dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/WA_DB/GPS-Raw.db"
study.sheep.tname <- "Yakima_Cleman_StudySheep"
cdep.tname <- "Yakima_Cleman_Collar_Deployment"
cap.tname <- "Yakima_Cleman_Capture"
ser.tname <- "Yakima_Cleman_Serology"
#pcr.tname <- "Yakima_Cleman_PCR_Status" # this table contains unique Animal/Date combos. second swabs have been resolved
#   this table is created using "writePCRStatus_toDB.R" to deal with indeterminate first swabs and condition on the second result
pcr.tname <- "Yakima_Cleman_Bacteriology"
offset <- 1 * 60 * 60 * 24 # number of days to buffer the start/stop to account for any mismatch
#  between and start/end times to prevent fixes on road, helicopter, offices
# if we're confident in the collar deployment times this can be made to hours or minutes?
#offset <- 60 * 60 * 2 # number of hours to buffer the start/stop to account for any mismatch
#  between and start/end times to prevent fixes on road, helicopter, offices
# if we're confident in the collar deployment times this can be made to hours or minutes?
# table name to write in main DB
all.name <- "AnimalID_GPS"
# The output table names we're reading from in raw gps DB
vec.table.name <- "VectronicGPS_All"
#tel.table.name <- "TelonicsGPS_All"
# -------- ---------------------------------------------------------------
# For the crontab log file
print(Sys.time())
#
# Connect to the data base #
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
gps.con <- dbConnect(RSQLite::SQLite(),gps.db.path, extended_types=TRUE)
# R script to read the manufacturer-based tables in the DB (serialNos only) and
#  create a unified GPS table for all AnimalIDs based on our deployment table records
#
#   written 2/23/24 to update AnimalID_GPS_toDB.R
#
#   revised 4/19/24 to deal with appending values to 'ended' tables that have location float round-off errors for same fix times
#     doing away with the 'ended' and 'active' table postfixes and added the EndType code to the table for query/filter
library(RSQLite)
library(dplyr)
# SETTINGS:  User-supplied inputs, update for your use
# Full path to main testing, finished GPS db
dbpath <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/WA_DB/BHS_WA.db"
# db to house raw GPS
gps.db.path <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/WA_DB/GPS-Raw.db"
study.sheep.tname <- "Yakima_Cleman_StudySheep"
cdep.tname <- "Yakima_Cleman_Collar_Deployment"
cap.tname <- "Yakima_Cleman_Capture"
ser.tname <- "Yakima_Cleman_Serology"
#pcr.tname <- "Yakima_Cleman_PCR_Status" # this table contains unique Animal/Date combos. second swabs have been resolved
#   this table is created using "writePCRStatus_toDB.R" to deal with indeterminate first swabs and condition on the second result
pcr.tname <- "Yakima_Cleman_Bacteriology"
offset <- 1 * 60 * 60 * 24 # number of days to buffer the start/stop to account for any mismatch
#  between and start/end times to prevent fixes on road, helicopter, offices
# if we're confident in the collar deployment times this can be made to hours or minutes?
#offset <- 60 * 60 * 2 # number of hours to buffer the start/stop to account for any mismatch
#  between and start/end times to prevent fixes on road, helicopter, offices
# if we're confident in the collar deployment times this can be made to hours or minutes?
# table name to write in main DB
all.name <- "AnimalID_GPS"
# The output table names we're reading from in raw gps DB
vec.table.name <- "VectronicGPS_All"
#tel.table.name <- "TelonicsGPS_All"
# -------- ---------------------------------------------------------------
# For the crontab log file
print(Sys.time())
#
# Connect to the data base #
con <- dbConnect(RSQLite::SQLite(),dbpath, extended_types=TRUE)
gps.con <- dbConnect(RSQLite::SQLite(),gps.db.path, extended_types=TRUE)
# read a table into a dataframe
vec.gps <- dbReadTable(gps.con,vec.table.name)
ss <- dbReadTable(con,study.sheep.tname)
cdep <- dbReadTable(con,cdep.tname)
cap.orig <- dbReadTable(con,cap.tname)
pcr <- dbReadTable(con,pcr.tname)
ser <- dbReadTable(con,ser.tname)
#force the serial numbers to character types
vec.gps$idcollar <- as.character(vec.gps$idcollar)
# join the sample tables to capture table
cap1 <- cap.orig %>% left_join(ser, join_by(AnimalID==AnimalID, CaptureDate==SampleDate))
cap2 <- cap1 %>% left_join(pcr, join_by(AnimalID==AnimalID, CaptureDate==SampleDate))
cap <- cap2 %>% select(AnimalID,CaptureDate,CaptureMethod, Latitude, Longitude, Sex, AgeClass,Herd,Movi_Elisa,Movi_Status, MoviPCR)
# some require formatting of date fields and such #
# format the date fields into an R posixct object
# need to protect against start/end dates w/out H:M or accidental entries of bad date formats
candidates <- c("%Y-%m-%d", "%d/%m/%Y %H:%M:%S", "%d/%m/%Y %H:%M", "%d/%m/%Y")
out <- as.POSIXct(cdep$Collar_start, format = "%Y-%m-%d %H:%M", tz="GMT")
for (fmt in candidates) {
if (!length(isna <- is.na(out))) break
out[isna] <- as.POSIXct(cdep$Collar_start[isna], format = fmt)
}
cdep$Collar_start <- out
out <- as.POSIXct(cdep$Collar_end, format = "%Y-%m-%d %H:%M", tz="GMT")
for (fmt in candidates) {
if (!length(isna <- is.na(out))) break
out[isna] <- as.POSIXct(cdep$Collar_end[isna], format = fmt)
}
cdep$Collar_end <- out
cdep.sub <- cdep %>% filter(CollarType=="GPS")
# gps time also needs to be converted to POSIXct from their mfg formats #
vec.gps$acquisitiontime <- as.POSIXct(strptime(vec.gps$acquisitiontime,format="%Y-%m-%dT%H:%M:%S",tz="GMT"))
# make a unified GPS data set with common column names and formats
gps <- data.frame(Serialnumber=c(vec.gps$idcollar),
acquisitiontime=c(vec.gps$acquisitiontime),
Latitude=c(vec.gps$latitude),
Longitude=c(vec.gps$longitude))
# loop through the deploy table and retrieve corresponding GPS data, effective but not elegant #
print(paste("Processing", nrow(cdep.sub),"GPS collar deployment records."))
for (i in 1:nrow(cdep.sub)) {
# details on this sheep from ss, pcr, and ser tables #
ind <- match(cdep.sub$AnimalID[i],ss$AnimalID)
sheep <- ss[ind,]
ind.cap <- match(cdep.sub$AnimalID[i],cap$AnimalID) # note this gets first match in table (ignore rcap when same collar is on)
inds <- which(cap$AnimalID %in% cdep.sub$AnimalID[i], arr.ind=TRUE) # gets all matches so we can get most recent Movi status for recaps
l.inds <- length(inds)
if (l.inds > 0) {
cap.index <- inds[l.inds]
#sheep.cap <- cap[ind.cap,] # gets first capture with this collar
sheep.cap <- cap[cap.index,] # should get the most recent capture (chooses last record in all for AnimalID whether >1 or not)
gps.i <- gps %>% filter(Serialnumber==cdep.sub$Serialnumber[i])  # match serial numbers
if (nrow(gps.i)>0){
if(cdep.sub$End_Type[i]==0) gps.i <- gps.i %>% filter(acquisitiontime > (cdep.sub$Collar_start[i]+offset))
if(!(cdep.sub$End_Type[i]==0)) gps.i <- gps.i %>% filter(acquisitiontime > (cdep.sub$Collar_start[i]+offset) & acquisitiontime < (cdep.sub$Collar_end[i]-offset))
# things we want to add to the GPS file, like AnimalID or Collar Frequency or Herd, or Sex?
gps.i$AnimalID <- rep(cdep.sub$AnimalID[i],nrow(gps.i))
gps.i$Frequency <- rep(cdep.sub$Frequency[i],nrow(gps.i))
gps.i$Herd <- rep(sheep$Herd,nrow(gps.i))
gps.i$Sex <- rep(sheep$Sex,nrow(gps.i))
gps.i$EntryBioYear <- rep(sheep$EntryBioYear,nrow(gps.i))
gps.i$CapturePCRStatus <- rep(sheep.cap$MoviPCR,nrow(gps.i))
gps.i$CaptureELISAStatus <- rep(sheep.cap$Movi_Status,nrow(gps.i))
gps.i$Capture_cELISA <- rep(sheep.cap$Movi_Elisa,nrow(gps.i))
gps.i$CollarEndType <- rep(cdep.sub$End_Type[i],nrow(gps.i))
# move these new columns towards the "front"
gps.i <- relocate(gps.i, Sex, .before=acquisitiontime)
gps.i <- relocate(gps.i, Herd, .before = Sex)
gps.i <- relocate(gps.i, Frequency, .before = Herd)
gps.i <- relocate(gps.i, AnimalID, .before = Frequency)
gps.i <- relocate(gps.i, CapturePCRStatus, .before=acquisitiontime)
gps.i <- relocate(gps.i, CaptureELISAStatus, .before=acquisitiontime)
gps.i <- relocate(gps.i, Capture_cELISA, .before=acquisitiontime)
} else print(paste(cdep.sub$AnimalID[i],"with Serial no:",cdep.sub$Serialnumber[i],"not matched in GPS tables"))
if (i==1){
out.data <- gps.i
} else out.data <- rbind(out.data,gps.i)   # build a data frame with all animals
} # end if statement for matching deployment to capture
if (l.inds == 0){
print(paste(cdep.sub$AnimalID[i],"from deployment table not found in capture table."))
}
} # end collar construct for loop of GPS data
out.data$acquisitiontime <- format(out.data$acquisitiontime, format="%Y-%m-%d %H:%M:%S",tz="GMT")
dbWriteTable(con, all.name, out.data,overwrite=TRUE)
# close DB connections
dbDisconnect(con)
dbDisconnect(gps.con)
# simple script to copy the database from Box (Cloud) to Local
cloud.db <- "/Users/scottp/Library/CloudStorage/Box-Box/WSF-GIA-FY22/Database/BHS_TriState.db"
local.db <- "/Users/scottp/DocumentsNew/BighornSheep/FY22-WSF-GIA/Databases/BHS_TriState.db"
print(Sys.time())
#file.copy(cloud.db,local.db, overwrite=TRUE)
file.copy(local.db,cloud.db, overwrite=TRUE)
print(paste("Finished copying",local.db,"to",cloud.db))
